{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.4 64-bit ('base': conda)"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"interpreter":{"hash":"344443636c3027c5042750c9c609acdda283a9c43681b128a8c1053e7ad2aa7d"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","import pandas as pd\n"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-01T17:12:39.787802Z","iopub.execute_input":"2021-10-01T17:12:39.788207Z","iopub.status.idle":"2021-10-01T17:12:39.804162Z","shell.execute_reply.started":"2021-10-01T17:12:39.788148Z","shell.execute_reply":"2021-10-01T17:12:39.802438Z"},"trusted":true}},{"cell_type":"code","execution_count":2,"source":["# import sys, os\n","# import pandas as pd\n","# import numpy as np\n","\n","# from sklearn.linear_model import Lasso\n","# from sklearn.feature_selection import RFECV\n","# from sklearn.preprocessing import RobustScaler\n","# from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n","# from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, r2_score, make_scorer\n","# from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","\n","# def warn(*args, **kwargs):\n","#     pass\n","# import warnings\n","# warnings.warn = warn\n","\n","# # some heuristic settings\n","# rfe_min_features = 12\n","# rfe_step = 15\n","# rfe_cv = 20\n","# sss_n_splits = 20\n","# sss_test_size = 0.35\n","# grid_search_cv = 20\n","# noise_std = 0.01\n","# r2_threshold = 0.185\n","# random_seed = 213\n","\n","# np.random.seed(random_seed)\n","\n","# # import data\n","# train = pd.read_csv('../input/dont-overfit-ii/train.csv')\n","# train_y = train['target']\n","# train_X = train.drop(['id','target'], axis=1).values\n","\n","# test = pd.read_csv('../input/dont-overfit-ii/test.csv')\n","# test = test.drop(['id'], axis=1).values\n","\n","# # scale using RobustScaler\n","# # fitting scaler on full data outperforms fitting on test_X only (+0.006 kaggle score)\n","# data = RobustScaler().fit_transform(np.concatenate((train_X, test), axis=0))\n","# train_X = data[:250]\n","# test = data[250:]\n","\n","# # add a bit of noise to train_X to reduce overfitting\n","# train_X += np.random.normal(0, noise_std, train_X.shape)\n","\n","# # define roc_auc_metric robust to only one class in y_pred\n","# def scoring_roc_auc(y, y_pred):\n","#     try:\n","#         return roc_auc_score(y, y_pred)\n","#     except:\n","#         return 0.5\n","\n","# robust_roc_auc = make_scorer(scoring_roc_auc)\n","\n","# # define model and its parameters\n","# model = Lasso(alpha=0.031, tol=0.01, random_state=random_seed, selection='random')\n","\n","# param_grid = {\n","#             'alpha' : [0.022, 0.021, 0.02, 0.019, 0.023, 0.024, 0.025, 0.026, 0.027, 0.029, 0.031],\n","#             'tol'   : [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012, 0.0016, 0.0017]\n","#         }\n","\n","# # define recursive elimination feature selector\n","# feature_selector = RFECV(model, min_features_to_select=rfe_min_features, scoring=robust_roc_auc, step=rfe_step, verbose=0, cv=rfe_cv, n_jobs=-1)\n","\n","# print(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\n","# print(\"-------------------------------------------------------------------------------------------------\")\n","\n","# predictions = pd.DataFrame()\n","# counter = 0\n","# # split training data to build one model on each traing-data-subset\n","# for train_index, val_index in StratifiedShuffleSplit(n_splits=sss_n_splits, test_size=sss_test_size, random_state=random_seed).split(train_X, train_y):\n","#     X, val_X = train_X[train_index], train_X[val_index]\n","#     y, val_y = train_y[train_index], train_y[val_index]\n","\n","#     # get the best features for this data set\n","#     feature_selector.fit(X, y)\n","#     # remove irrelevant features from X, val_X and test\n","#     X_important_features        = feature_selector.transform(X)\n","#     val_X_important_features    = feature_selector.transform(val_X)\n","#     test_important_features     = feature_selector.transform(test)\n","\n","#     # run grid search to find the best Lasso parameters for this subset of training data and subset of features \n","#     grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\n","#     grid_search.fit(X_important_features, y)\n","\n","#     # score our fitted model on validation data\n","#     val_y_pred = grid_search.best_estimator_.predict(val_X_important_features)\n","#     val_mse = mean_squared_error(val_y, val_y_pred)\n","#     val_mae = mean_absolute_error(val_y, val_y_pred)\n","#     val_roc = roc_auc_score(val_y, val_y_pred)\n","#     val_cos = cosine_similarity(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n","#     val_dst = euclidean_distances(val_y.values.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n","#     val_r2  = r2_score(val_y, val_y_pred)\n","\n","#     # if model did well on validation, save its prediction on test data, using only important features\n","#     # r2_threshold (0.185) is a heuristic threshold for r2 error\n","#     # you can use any other metric/metric combination that works for you\n","#     if val_r2 > r2_threshold:\n","#         message = '<-- OK'\n","#         prediction = grid_search.best_estimator_.predict(test_important_features)\n","#         predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n","#     else:\n","#         message = '<-- skipping'\n","\n","\n","#     print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n","    \n","#     counter += 1\n","\n","# print(\"-------------------------------------------------------------------------------------------------\")\n","# print(\"{}/{} models passed validation threshold and will be ensembled.\".format(len(predictions.columns), sss_n_splits))\n","\n","# mean_pred = pd.DataFrame(predictions.mean(axis=1))\n","# mean_pred.index += 250\n","# mean_pred.columns = ['target']\n","# mean_pred.to_csv('submission_15.csv', index_label='id', index=True) "],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:16:28.395486Z","iopub.execute_input":"2021-10-01T17:16:28.395815Z","iopub.status.idle":"2021-10-01T17:17:28.717791Z","shell.execute_reply.started":"2021-10-01T17:16:28.395784Z","shell.execute_reply":"2021-10-01T17:17:28.716607Z"},"trusted":true}},{"cell_type":"code","execution_count":3,"source":["train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:12:39.809745Z","iopub.execute_input":"2021-10-01T17:12:39.809985Z","iopub.status.idle":"2021-10-01T17:12:40.917437Z","shell.execute_reply.started":"2021-10-01T17:12:39.809958Z","shell.execute_reply":"2021-10-01T17:12:40.916032Z"},"trusted":true}},{"cell_type":"markdown","source":["## Data Exploration"],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["train.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>-0.098</td>\n","      <td>2.165</td>\n","      <td>0.681</td>\n","      <td>-0.614</td>\n","      <td>1.309</td>\n","      <td>-0.455</td>\n","      <td>-0.236</td>\n","      <td>0.276</td>\n","      <td>...</td>\n","      <td>0.867</td>\n","      <td>1.347</td>\n","      <td>0.504</td>\n","      <td>-0.649</td>\n","      <td>0.672</td>\n","      <td>-2.097</td>\n","      <td>1.051</td>\n","      <td>-0.414</td>\n","      <td>1.038</td>\n","      <td>-1.065</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.081</td>\n","      <td>-0.973</td>\n","      <td>-0.383</td>\n","      <td>0.326</td>\n","      <td>-0.428</td>\n","      <td>0.317</td>\n","      <td>1.172</td>\n","      <td>0.352</td>\n","      <td>...</td>\n","      <td>-0.165</td>\n","      <td>-1.695</td>\n","      <td>-1.257</td>\n","      <td>1.359</td>\n","      <td>-0.808</td>\n","      <td>-1.624</td>\n","      <td>-0.458</td>\n","      <td>-1.099</td>\n","      <td>-0.936</td>\n","      <td>0.973</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>-0.523</td>\n","      <td>-0.089</td>\n","      <td>-0.348</td>\n","      <td>0.148</td>\n","      <td>-0.022</td>\n","      <td>0.404</td>\n","      <td>-0.023</td>\n","      <td>-0.172</td>\n","      <td>...</td>\n","      <td>0.013</td>\n","      <td>0.263</td>\n","      <td>-1.222</td>\n","      <td>0.726</td>\n","      <td>1.444</td>\n","      <td>-1.165</td>\n","      <td>-1.544</td>\n","      <td>0.004</td>\n","      <td>0.800</td>\n","      <td>-1.211</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>0.067</td>\n","      <td>-0.021</td>\n","      <td>0.392</td>\n","      <td>-1.637</td>\n","      <td>-0.446</td>\n","      <td>-0.725</td>\n","      <td>-1.035</td>\n","      <td>0.834</td>\n","      <td>...</td>\n","      <td>-0.404</td>\n","      <td>0.640</td>\n","      <td>-0.595</td>\n","      <td>-0.966</td>\n","      <td>0.900</td>\n","      <td>0.467</td>\n","      <td>-0.562</td>\n","      <td>-0.254</td>\n","      <td>-0.533</td>\n","      <td>0.238</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1.0</td>\n","      <td>2.347</td>\n","      <td>-0.831</td>\n","      <td>0.511</td>\n","      <td>-0.021</td>\n","      <td>1.225</td>\n","      <td>1.594</td>\n","      <td>0.585</td>\n","      <td>1.509</td>\n","      <td>...</td>\n","      <td>0.898</td>\n","      <td>0.134</td>\n","      <td>2.415</td>\n","      <td>-0.996</td>\n","      <td>-1.006</td>\n","      <td>1.378</td>\n","      <td>1.246</td>\n","      <td>1.478</td>\n","      <td>0.428</td>\n","      <td>0.253</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 302 columns</p>\n","</div>"],"text/plain":["   id  target      0      1      2      3      4      5      6      7  ...  \\\n","0   0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276  ...   \n","1   1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  ...   \n","2   2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  ...   \n","3   3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  ...   \n","4   4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509  ...   \n","\n","     290    291    292    293    294    295    296    297    298    299  \n","0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n","1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n","2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n","3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n","4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n","\n","[5 rows x 302 columns]"]},"metadata":{},"execution_count":4}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":5,"source":["print(train.shape, test.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(250, 302) (19750, 301)\n"]}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":6,"source":["train.describe()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>...</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","      <td>250.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>124.500000</td>\n","      <td>0.640000</td>\n","      <td>0.023292</td>\n","      <td>-0.026872</td>\n","      <td>0.167404</td>\n","      <td>0.001904</td>\n","      <td>0.001588</td>\n","      <td>-0.007304</td>\n","      <td>0.032052</td>\n","      <td>0.078412</td>\n","      <td>...</td>\n","      <td>0.044652</td>\n","      <td>0.126344</td>\n","      <td>0.018436</td>\n","      <td>-0.012092</td>\n","      <td>-0.065720</td>\n","      <td>-0.106112</td>\n","      <td>0.046472</td>\n","      <td>0.006452</td>\n","      <td>0.009372</td>\n","      <td>-0.128952</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>72.312977</td>\n","      <td>0.480963</td>\n","      <td>0.998354</td>\n","      <td>1.009314</td>\n","      <td>1.021709</td>\n","      <td>1.011751</td>\n","      <td>1.035411</td>\n","      <td>0.955700</td>\n","      <td>1.006657</td>\n","      <td>0.939731</td>\n","      <td>...</td>\n","      <td>1.011416</td>\n","      <td>0.972567</td>\n","      <td>0.954229</td>\n","      <td>0.960630</td>\n","      <td>1.057414</td>\n","      <td>1.038389</td>\n","      <td>0.967661</td>\n","      <td>0.998984</td>\n","      <td>1.008099</td>\n","      <td>0.971219</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-2.319000</td>\n","      <td>-2.931000</td>\n","      <td>-2.477000</td>\n","      <td>-2.359000</td>\n","      <td>-2.566000</td>\n","      <td>-2.845000</td>\n","      <td>-2.976000</td>\n","      <td>-3.444000</td>\n","      <td>...</td>\n","      <td>-2.804000</td>\n","      <td>-2.443000</td>\n","      <td>-2.757000</td>\n","      <td>-2.466000</td>\n","      <td>-3.287000</td>\n","      <td>-3.072000</td>\n","      <td>-2.634000</td>\n","      <td>-2.776000</td>\n","      <td>-3.211000</td>\n","      <td>-3.500000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>62.250000</td>\n","      <td>0.000000</td>\n","      <td>-0.644750</td>\n","      <td>-0.739750</td>\n","      <td>-0.425250</td>\n","      <td>-0.686500</td>\n","      <td>-0.659000</td>\n","      <td>-0.643750</td>\n","      <td>-0.675000</td>\n","      <td>-0.550750</td>\n","      <td>...</td>\n","      <td>-0.617000</td>\n","      <td>-0.510500</td>\n","      <td>-0.535750</td>\n","      <td>-0.657000</td>\n","      <td>-0.818500</td>\n","      <td>-0.821000</td>\n","      <td>-0.605500</td>\n","      <td>-0.751250</td>\n","      <td>-0.550000</td>\n","      <td>-0.754250</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>124.500000</td>\n","      <td>1.000000</td>\n","      <td>-0.015500</td>\n","      <td>0.057000</td>\n","      <td>0.184000</td>\n","      <td>-0.016500</td>\n","      <td>-0.023000</td>\n","      <td>0.037500</td>\n","      <td>0.060500</td>\n","      <td>0.183500</td>\n","      <td>...</td>\n","      <td>0.067500</td>\n","      <td>0.091000</td>\n","      <td>0.057500</td>\n","      <td>-0.021000</td>\n","      <td>-0.009000</td>\n","      <td>-0.079500</td>\n","      <td>0.009500</td>\n","      <td>0.005500</td>\n","      <td>-0.009000</td>\n","      <td>-0.132500</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>186.750000</td>\n","      <td>1.000000</td>\n","      <td>0.677000</td>\n","      <td>0.620750</td>\n","      <td>0.805000</td>\n","      <td>0.720000</td>\n","      <td>0.735000</td>\n","      <td>0.660500</td>\n","      <td>0.783250</td>\n","      <td>0.766250</td>\n","      <td>...</td>\n","      <td>0.797250</td>\n","      <td>0.804250</td>\n","      <td>0.631500</td>\n","      <td>0.650250</td>\n","      <td>0.739500</td>\n","      <td>0.493000</td>\n","      <td>0.683000</td>\n","      <td>0.794250</td>\n","      <td>0.654250</td>\n","      <td>0.503250</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>249.000000</td>\n","      <td>1.000000</td>\n","      <td>2.567000</td>\n","      <td>2.419000</td>\n","      <td>3.392000</td>\n","      <td>2.771000</td>\n","      <td>2.901000</td>\n","      <td>2.793000</td>\n","      <td>2.546000</td>\n","      <td>2.846000</td>\n","      <td>...</td>\n","      <td>2.865000</td>\n","      <td>2.801000</td>\n","      <td>2.736000</td>\n","      <td>2.596000</td>\n","      <td>2.226000</td>\n","      <td>3.131000</td>\n","      <td>3.236000</td>\n","      <td>2.626000</td>\n","      <td>3.530000</td>\n","      <td>2.771000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 302 columns</p>\n","</div>"],"text/plain":["               id      target           0           1           2           3  \\\n","count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n","mean   124.500000    0.640000    0.023292   -0.026872    0.167404    0.001904   \n","std     72.312977    0.480963    0.998354    1.009314    1.021709    1.011751   \n","min      0.000000    0.000000   -2.319000   -2.931000   -2.477000   -2.359000   \n","25%     62.250000    0.000000   -0.644750   -0.739750   -0.425250   -0.686500   \n","50%    124.500000    1.000000   -0.015500    0.057000    0.184000   -0.016500   \n","75%    186.750000    1.000000    0.677000    0.620750    0.805000    0.720000   \n","max    249.000000    1.000000    2.567000    2.419000    3.392000    2.771000   \n","\n","                4           5           6           7  ...         290  \\\n","count  250.000000  250.000000  250.000000  250.000000  ...  250.000000   \n","mean     0.001588   -0.007304    0.032052    0.078412  ...    0.044652   \n","std      1.035411    0.955700    1.006657    0.939731  ...    1.011416   \n","min     -2.566000   -2.845000   -2.976000   -3.444000  ...   -2.804000   \n","25%     -0.659000   -0.643750   -0.675000   -0.550750  ...   -0.617000   \n","50%     -0.023000    0.037500    0.060500    0.183500  ...    0.067500   \n","75%      0.735000    0.660500    0.783250    0.766250  ...    0.797250   \n","max      2.901000    2.793000    2.546000    2.846000  ...    2.865000   \n","\n","              291         292         293         294         295         296  \\\n","count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n","mean     0.126344    0.018436   -0.012092   -0.065720   -0.106112    0.046472   \n","std      0.972567    0.954229    0.960630    1.057414    1.038389    0.967661   \n","min     -2.443000   -2.757000   -2.466000   -3.287000   -3.072000   -2.634000   \n","25%     -0.510500   -0.535750   -0.657000   -0.818500   -0.821000   -0.605500   \n","50%      0.091000    0.057500   -0.021000   -0.009000   -0.079500    0.009500   \n","75%      0.804250    0.631500    0.650250    0.739500    0.493000    0.683000   \n","max      2.801000    2.736000    2.596000    2.226000    3.131000    3.236000   \n","\n","              297         298         299  \n","count  250.000000  250.000000  250.000000  \n","mean     0.006452    0.009372   -0.128952  \n","std      0.998984    1.008099    0.971219  \n","min     -2.776000   -3.211000   -3.500000  \n","25%     -0.751250   -0.550000   -0.754250  \n","50%      0.005500   -0.009000   -0.132500  \n","75%      0.794250    0.654250    0.503250  \n","max      2.626000    3.530000    2.771000  \n","\n","[8 rows x 302 columns]"]},"metadata":{},"execution_count":6}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":7,"source":["train.info()"],"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 250 entries, 0 to 249\n","Columns: 302 entries, id to 299\n","dtypes: float64(301), int64(1)\n","memory usage: 590.0 KB\n"]}],"metadata":{"trusted":true}},{"cell_type":"markdown","source":[" A classification model. With 299 features and 250 data points on the train. The test has 19750 data points. Values are not between 0 and 1, so might need some scaling and then PCA. Using a pipeline will be a good approach.\n"," \n"," Datatypes are mostly float and only one int."],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["# Lets check for missing values\n","print(f\"Missing values in train sum: \",train.isnull().any().sum())\n","print(f\"Missing values in test sum: \",test.isnull().any().sum())"],"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values in train sum:  0\n","Missing values in test sum:  0\n"]}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":9,"source":["print(\"Value counts of target in train set:\")\n","train[\"target\"].value_counts()\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Value counts of target in train set:\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0    160\n","0.0     90\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":9}],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["Target value is not balanced. Accuracy metric should not be considered."],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["print(\"20 most correlated features to target variable:\")\n","corr = train.corr().target.sort_values(ascending=False)\n","corr[:21]"],"outputs":[{"output_type":"stream","name":"stdout","text":["20 most correlated features to target variable:\n"]},{"output_type":"execute_result","data":{"text/plain":["target    1.000000\n","33        0.373608\n","65        0.293846\n","24        0.173096\n","183       0.164146\n","199       0.159442\n","201       0.142238\n","30        0.132705\n","289       0.127213\n","114       0.124792\n","164       0.124151\n","101       0.118379\n","272       0.113909\n","226       0.113660\n","17        0.110998\n","105       0.110589\n","0         0.108966\n","244       0.108147\n","13        0.107828\n","176       0.099790\n","89        0.099526\n","Name: target, dtype: float64"]},"metadata":{},"execution_count":10}],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["## Baseline Model"],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, RobustScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score,confusion_matrix\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report\n","seed = 42"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:12:46.079758Z","iopub.execute_input":"2021-10-01T17:12:46.080004Z","iopub.status.idle":"2021-10-01T17:12:46.431122Z","shell.execute_reply.started":"2021-10-01T17:12:46.079981Z","shell.execute_reply":"2021-10-01T17:12:46.42993Z"},"trusted":true}},{"cell_type":"code","execution_count":12,"source":["test.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>250</td>\n","      <td>0.500</td>\n","      <td>-1.033</td>\n","      <td>-1.595</td>\n","      <td>0.309</td>\n","      <td>-0.714</td>\n","      <td>0.502</td>\n","      <td>0.535</td>\n","      <td>-0.129</td>\n","      <td>-0.687</td>\n","      <td>...</td>\n","      <td>-0.088</td>\n","      <td>-2.628</td>\n","      <td>-0.845</td>\n","      <td>2.078</td>\n","      <td>-0.277</td>\n","      <td>2.132</td>\n","      <td>0.609</td>\n","      <td>-0.104</td>\n","      <td>0.312</td>\n","      <td>0.979</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>251</td>\n","      <td>0.776</td>\n","      <td>0.914</td>\n","      <td>-0.494</td>\n","      <td>1.347</td>\n","      <td>-0.867</td>\n","      <td>0.480</td>\n","      <td>0.578</td>\n","      <td>-0.313</td>\n","      <td>0.203</td>\n","      <td>...</td>\n","      <td>-0.683</td>\n","      <td>-0.066</td>\n","      <td>0.025</td>\n","      <td>0.606</td>\n","      <td>-0.353</td>\n","      <td>-1.133</td>\n","      <td>-3.138</td>\n","      <td>0.281</td>\n","      <td>-0.625</td>\n","      <td>-0.761</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>252</td>\n","      <td>1.750</td>\n","      <td>0.509</td>\n","      <td>-0.057</td>\n","      <td>0.835</td>\n","      <td>-0.476</td>\n","      <td>1.428</td>\n","      <td>-0.701</td>\n","      <td>-2.009</td>\n","      <td>-1.378</td>\n","      <td>...</td>\n","      <td>-0.094</td>\n","      <td>0.351</td>\n","      <td>-0.607</td>\n","      <td>-0.737</td>\n","      <td>-0.031</td>\n","      <td>0.701</td>\n","      <td>0.976</td>\n","      <td>0.135</td>\n","      <td>-1.327</td>\n","      <td>2.463</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>253</td>\n","      <td>-0.556</td>\n","      <td>-1.855</td>\n","      <td>-0.682</td>\n","      <td>0.578</td>\n","      <td>1.592</td>\n","      <td>0.512</td>\n","      <td>-1.419</td>\n","      <td>0.722</td>\n","      <td>0.511</td>\n","      <td>...</td>\n","      <td>-0.336</td>\n","      <td>-0.787</td>\n","      <td>0.255</td>\n","      <td>-0.031</td>\n","      <td>-0.836</td>\n","      <td>0.916</td>\n","      <td>2.411</td>\n","      <td>1.053</td>\n","      <td>-1.601</td>\n","      <td>-1.529</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>254</td>\n","      <td>0.754</td>\n","      <td>-0.245</td>\n","      <td>1.173</td>\n","      <td>-1.623</td>\n","      <td>0.009</td>\n","      <td>0.370</td>\n","      <td>0.781</td>\n","      <td>-1.763</td>\n","      <td>-1.432</td>\n","      <td>...</td>\n","      <td>2.184</td>\n","      <td>-1.090</td>\n","      <td>0.216</td>\n","      <td>1.186</td>\n","      <td>-0.143</td>\n","      <td>0.322</td>\n","      <td>-0.068</td>\n","      <td>-0.156</td>\n","      <td>-1.153</td>\n","      <td>0.825</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 301 columns</p>\n","</div>"],"text/plain":["    id      0      1      2      3      4      5      6      7      8  ...  \\\n","0  250  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  ...   \n","1  251  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  ...   \n","2  252  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  ...   \n","3  253 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  ...   \n","4  254  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432  ...   \n","\n","     290    291    292    293    294    295    296    297    298    299  \n","0 -0.088 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n","1 -0.683 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n","2 -0.094  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n","3 -0.336 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n","4  2.184 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  \n","\n","[5 rows x 301 columns]"]},"metadata":{},"execution_count":12}],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["\n","train_y = train['target']\n","train_x = train.drop(['id', 'target'], axis=1)\n","\n","test = test.drop(['id'], axis=1)\n","scaler = StandardScaler()\n","X = train_x.copy()\n","y = train_y.copy()\n","X_scaled=scaler.fit_transform(X)\n","#train_df = X_scaled.copy()\n","y_df = y.copy()\n","test_scaled =scaler.transform(test)\n","# X_train_scaled,X_val_scaled,y_train_scaled,y_val_scaled = train_test_split(X_scaled,y, test_size=0.2,stratify=y)\n","# X_train,X_val,y_train,y_val = train_test_split(X,y, test_size=0.2,stratify=y)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:12:53.696885Z","iopub.execute_input":"2021-10-01T17:12:53.697206Z","iopub.status.idle":"2021-10-01T17:12:53.716497Z","shell.execute_reply.started":"2021-10-01T17:12:53.697175Z","shell.execute_reply":"2021-10-01T17:12:53.715525Z"},"trusted":true}},{"cell_type":"code","execution_count":14,"source":["logistic = LogisticRegression(random_state=seed)\n","pipe = Pipeline([\n","                (\"scaler\", StandardScaler()),\n","                (\"model\", logistic)   \n","                ])\n","pipe.fit(train_x,train_y)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('scaler', StandardScaler()),\n","                ('model', LogisticRegression(random_state=42))])"]},"metadata":{},"execution_count":14}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":16,"source":["print(f\"Train score:\",pipe.score(train_x,train_y))\n","# print(f\"Test score:\",pipe.score(X_val, y_val))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Train score: 1.0\n"]}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# y_pred = pipe.predict(X_val)\n","# print(classification_report(y_val, y_pred, target_names=['0', '1']))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# print(confusion_matrix(y_val, y_pred))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["The model did poorly in classifying the model. On the leaderboard, it scored 0.51."],"metadata":{}},{"cell_type":"markdown","source":["## Logistic Regression with L1 regularization of various strength"],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["train_x = RobustScaler().fit_transform(train_x)\n","noise_std = 0.01\n","train_x += np.random.normal(0, noise_std, train_x.shape)\n","test = RobustScaler().fit_transform(test)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:12:58.843785Z","iopub.execute_input":"2021-10-01T17:12:58.844031Z","iopub.status.idle":"2021-10-01T17:12:59.171066Z","shell.execute_reply.started":"2021-10-01T17:12:58.844009Z","shell.execute_reply":"2021-10-01T17:12:59.169873Z"},"trusted":true}},{"cell_type":"code","execution_count":21,"source":["from sklearn.model_selection import cross_val_score\n","clf = LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\n","clf.fit(train_x, train_y)\n","print(f'5-fold val score : {cross_val_score(clf, train_x, train_y, cv=5)}')"],"outputs":[{"output_type":"stream","name":"stdout","text":["5-fold val score : [0.72 0.7  0.76 0.74 0.7 ]\n"]}],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:13:03.083709Z","iopub.execute_input":"2021-10-01T17:13:03.084591Z","iopub.status.idle":"2021-10-01T17:13:03.137057Z","shell.execute_reply.started":"2021-10-01T17:13:03.084545Z","shell.execute_reply":"2021-10-01T17:13:03.135752Z"},"trusted":true}},{"cell_type":"code","execution_count":22,"source":["clf.fit(train_x, train_y)\n","result = clf.predict_proba(test)\n","result"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.34743413, 0.65256587],\n","       [0.4918823 , 0.5081177 ],\n","       [0.50599509, 0.49400491],\n","       ...,\n","       [0.67504129, 0.32495871],\n","       [0.26886927, 0.73113073],\n","       [0.73147025, 0.26852975]])"]},"metadata":{},"execution_count":22}],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:13:06.524316Z","iopub.execute_input":"2021-10-01T17:13:06.52456Z","iopub.status.idle":"2021-10-01T17:13:06.544657Z","shell.execute_reply.started":"2021-10-01T17:13:06.524537Z","shell.execute_reply":"2021-10-01T17:13:06.543834Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["C = [10, 1, .1, .001]\n","\n","for c in C:\n","    log_l1 = LogisticRegression(penalty='l1', C=c, solver='liblinear',random_state=seed)\n","    \n","    pipe = Pipeline([\n","                (\"scaler\", StandardScaler()),\n","                (\"model\", log_l1)])\n","    pipe.fit(X_train, y_train)\n","    print('C:', c)\n","    print('Training accuracy:', pipe.score(train_x, y_train))\n","    print('Test accuracy:', pipe.score(X_val, y_val))\n","    print('')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["With an inverse of regularization strength C of 0.1 did better on both train and test set."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["log_l1_ = LogisticRegression(penalty='l1', C=1, solver='liblinear',random_state=seed)\n","pipe_log_l1 = Pipeline([\n","                (\"scaler\", StandardScaler()),\n","                (\"model\", log_l1_)   \n","                ])\n","pipe_log_l1.fit(X_train,y_train)\n","print(f\"Train score:\",pipe_log_l1.score(X_train, y_train))\n","print(f\"Test score:\",pipe_log_l1.score(X_val, y_val))\n","print(\"-------------------------------\")\n","y_pred_log_l1 = pipe_log_l1.predict(X_val)\n","print(classification_report(y_val, y_pred_log_l1, target_names=['0', '1']))\n","print(\"-------------------------------\")\n","print(confusion_matrix(y_val, y_pred_log_l1))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["This submission didn't do well on the leader board. it scored 0.4890. With C = 1 improved the leaderboard score to 0.495. None has done better than the base model. Let's try other advanced classifiers like Random Forest, CatBoost, XGBoost with optimizations."],"metadata":{}},{"cell_type":"markdown","source":["## One more thing - logistic regression with PCA"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html\n","# https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n","# Define a pipeline to search for the best combination of PCA truncation\n","# and classifier regularization.\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import GridSearchCV\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.ticker as ticker\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","for i in range(10, 200, 10):\n","    pca = PCA(n_components=i)\n","\n","    # set the tolerance to a large value to make the example faster\n","    logistic = LogisticRegression(solver='liblinear', tol=0.1, random_state=seed)\n","    pipe_log_pca = Pipeline([\n","                    (\"scaler\", StandardScaler()),\n","                    (\"pca\", pca),\n","                    (\"model\", logistic)   \n","                    ])\n","    pipe_log_pca.fit(X_train,y_train)\n","#     print(f\"Train score:\",pipe_log_pca.score(X_train, y_train))\n","#     print(f\"Test score:\",pipe_log_pca.score(X_val, y_val))\n","    print(\"-------------------------------\")\n","    print('n-components:', i)\n","    print('Training accuracy:', pipe_log_pca.score(X_train, y_train))\n","    print('Test accuracy:', pipe_log_pca.score(X_val, y_val))\n","    print('')\n","#     y_pred_log_pca = pipe_log_pca.predict(X_val)\n","#     print(classification_report(y_val, y_pred_log_pca, target_names=['0', '1']))\n","#     print(\"-------------------------------\")\n","#     print(confusion_matrix(y_val, y_pred_log_pca))\n","#     print(\"-------------------------------\")\n","#     print(np.cumsum(pca.explained_variance_ratio_))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["\n","pca = PCA(n_components=80)\n","\n","    # set the tolerance to a large value to make the example faster\n","logistic = LogisticRegression(solver='liblinear', tol=0.1, random_state=seed)\n","pipe_log_pca = Pipeline([\n","                    (\"scaler\", StandardScaler()),\n","                    (\"pca\", pca),\n","                    (\"model\", logistic)   \n","                    ])\n","pipe_log_pca.fit(X_train,y_train)\n","print(f\"Train score:\",pipe_log_pca.score(X_train, y_train))\n","print(f\"Test score:\",pipe_log_pca.score(X_val, y_val))\n","print(\"-------------------------------\")\n","#     print('n-components:', i)\n","#     print('Training accuracy:', pipe_log_pca.score(X_train, y_train))\n","#     print('Test accuracy:', pipe_log_pca.score(X_val, y_val))\n","#     print('')\n","y_pred_log_pca = pipe_log_pca.predict(X_val)\n","print(classification_report(y_val, y_pred_log_pca, target_names=['0', '1']))\n","print(\"-------------------------------\")\n","print(confusion_matrix(y_val, y_pred_log_pca))\n","print(\"-------------------------------\")\n","print(np.cumsum(pca.explained_variance_ratio_))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["With PCA, obtained a score of .471 on the leaderboard."],"metadata":{}},{"cell_type":"markdown","source":["## Random Forest Classifier"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from sklearn.ensemble import RandomForestClassifier\n","forest = RandomForestClassifier(random_state = seed)\n","modelF = forest.fit(X_train_scaled, y_train_scaled)\n","y_predF = modelF.predict(X_val_scaled)\n","print('Training accuracy:', modelF.score(X_train_scaled, y_train_scaled))\n","print('Test accuracy:', modelF.score(X_val_scaled, y_val_scaled))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["n_estimators = [100, 300, 500, 800, 1200]\n","max_depth = [5, 8, 15, 25, 30]\n","min_samples_split = [2, 5, 10, 15, 100]\n","min_samples_leaf = [1, 2, 5, 10] \n","\n","hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n","              min_samples_split = min_samples_split, \n","             min_samples_leaf = min_samples_leaf)\n","\n","gridF = GridSearchCV(forest, hyperF, cv = 5, verbose = 1, \n","                      n_jobs = -1)\n","bestF = gridF.fit(X_train_scaled, y_train_scaled)\n","bestF.best_score_"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["bestF.best_params_"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["forest_ = RandomForestClassifier(random_state = seed,max_depth= 5,min_samples_leaf= 1,min_samples_split=2,n_estimators=100)\n","modelF_ = forest_.fit(X_train_scaled, y_train_scaled)\n","y_predF_ = modelF_.predict(X_val_scaled)\n","print('Training accuracy:', modelF_.score(X_train_scaled, y_train_scaled))\n","print('Test accuracy:', modelF_.score(X_val_scaled, y_val_scaled))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["Random forest with no fine tuning had a score of 0.499 on the leaderboard. With Gridsearchcv gives a score of 0.501."],"metadata":{}},{"cell_type":"markdown","source":["## Trying out Loocv with Logistic Regression"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV , train_test_split , cross_val_score\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.model_selection import LeavePOut\n","loocv = LeaveOneOut()\n","model_loocv = LogisticRegression(class_weight='balanced', C= 0.1, max_iter=10000).fit(X_scaled, y)\n","results_loocv = cross_val_score(model_loocv, X_scaled, y, cv=loocv)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model_loocv.fit(X_scaled,y)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["## Feature selection using RFECV on Lasso"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# define roc_auc_metric \n","#then make it as ascorer \n","from sklearn.preprocessing import RobustScaler\n","robust_scale_X = RobustScaler().fit_transform(X)\n","robust_scale_test = RobustScaler().fit_transform(X_test)\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, r2_score, make_scorer\n","def scoring_roc_auc(y, y_pred):\n","    try:\n","        return roc_auc_score(y, y_pred)\n","    except:\n","        return 0.5\n","\n","robust_roc_auc = make_scorer(scoring_roc_auc)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from sklearn.linear_model import Lasso\n","from sklearn.feature_selection import RFECV\n","rfe_min_features = 12\n","rfe_step = 15\n","rfe_cv = 20\n","sss_n_splits = 20\n","sss_test_size = 0.35\n","grid_search_cv = 20\n","noise_std = 0.01\n","r2_threshold = 0.185\n","\n","model = Lasso(alpha=0.031, tol=0.01, warm_start=True, random_state=seed, selection='random')\n","param_grid = {\n","    'alpha': [0.022,0.021,0.02, 0.019, 0.023, 0.024, 0.025, 0.026, 0.027, 0.029, 0.031],\n","    'tol': [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012, 0.0016, 0.0017]}\n","# Define recursive elimination feature selector\n","feature_selector = RFECV(model, min_features_to_select=rfe_min_features, scoring=robust_roc_auc,\n","                        step=rfe_step, verbose=0, cv=rfe_cv, n_jobs=-1)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["print(train_df.shape)\n","print(y_df.shape)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","predictions = pd.DataFrame()\n","counter = 0\n","print(\"counter | val_mse  |  val_mae  |  val_roc  |  val_cos  |  val_dist  |  val_r2    | feature_count \")\n","print(\"-------------------------------------------------------------------------------------------------\")\n","# split training data to build one model on each traing-data-subset\n","for train_index, val_index in StratifiedShuffleSplit(n_splits=sss_n_splits, test_size=sss_test_size, random_state=seed).split(X_scaled, y):\n","    X, val_X = robust_scale_X[train_index], robust_scale_X[val_index]\n","    y, val_y = y_df[train_index], y_df[val_index]\n","\n","    # get the best features for this data set\n","    feature_selector.fit(X, y)\n","    # remove irrelevant features from X, val_X and test\n","    X_important_features        = feature_selector.transform(X)\n","    val_X_important_features    = feature_selector.transform(val_X)\n","    test_important_features     = feature_selector.transform(robust_scale_test)\n","\n","    # run grid search to find the best Lasso parameters for this subset of training data and subset of features \n","    grid_search = GridSearchCV(feature_selector.estimator_, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=robust_roc_auc, cv=20)\n","    grid_search.fit(X_important_features, y)\n","\n","    # score  fitted model on validation data\n","    val_y_pred = grid_search.best_estimator_.predict(val_X_important_features)\n","    val_mse = mean_squared_error(val_y, val_y_pred)\n","    val_mae = mean_absolute_error(val_y, val_y_pred)\n","    val_roc = roc_auc_score(val_y, val_y_pred)\n","    val_cos = cosine_similarity(val_y.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n","    val_dst = euclidean_distances(val_y.reshape(1, -1), val_y_pred.reshape(1, -1))[0][0]\n","    val_r2  = r2_score(val_y, val_y_pred)\n","\n","    # if model did well on validation, save its prediction on test data, using only important features\n","    # r2_threshold (0.185) is a heuristic threshold for r2 error\n","    # you can use any other metric/metric combination that works for you\n","    if val_r2 > r2_threshold:\n","        message = '<-- OK'\n","        prediction = grid_search.best_estimator_.predict(test_important_features)\n","        predictions = pd.concat([predictions, pd.DataFrame(prediction)], axis=1)\n","    else:\n","        message = '<-- skipping'\n","\n","\n","    print(\"{0:2}      | {1:.4f}   |  {2:.4f}   |  {3:.4f}   |  {4:.4f}   |  {5:.4f}    |  {6:.4f}    |  {7:3}         {8}  \".format(counter, val_mse, val_mae, val_roc, val_cos, val_dst, val_r2, feature_selector.n_features_, message))\n","    \n","    counter += 1\n","\n","print(\"-------------------------------------------------------------------------------------------------\")\n","print(\"{}/{} models passed validation threshold and will be ensembled.\".format(len(predictions.columns), sss_n_splits))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":24,"source":["submission = pd.read_csv('sample_submission.csv')\n","submission['target'] = clf.predict_proba(test)\n","submission.to_csv('submission_31.csv', index=False)\n","\n","submission.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>250</td>\n","      <td>0.347434</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>251</td>\n","      <td>0.491882</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>252</td>\n","      <td>0.505995</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>253</td>\n","      <td>0.340229</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>254</td>\n","      <td>0.536247</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id    target\n","0  250  0.347434\n","1  251  0.491882\n","2  252  0.505995\n","3  253  0.340229\n","4  254  0.536247"]},"metadata":{},"execution_count":24}],"metadata":{"execution":{"iopub.status.busy":"2021-10-01T17:13:25.263917Z","iopub.execute_input":"2021-10-01T17:13:25.264175Z","iopub.status.idle":"2021-10-01T17:13:25.353859Z","shell.execute_reply.started":"2021-10-01T17:13:25.264152Z","shell.execute_reply":"2021-10-01T17:13:25.353265Z"},"trusted":true}},{"cell_type":"code","execution_count":23,"source":["mean_pred = pd.DataFrame(predictions.mean(axis=1))\n","mean_pred.index += 250\n","mean_pred.columns = ['target']\n","mean_pred.to_csv('submission_31.csv', index_label='id', index=True) "],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'predictions' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-94b0974e9f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmean_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmean_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_31.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}